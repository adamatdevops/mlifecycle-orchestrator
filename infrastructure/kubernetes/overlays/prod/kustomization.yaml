# =============================================================================
# Production Environment Overlay
# =============================================================================

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: ml-inference-prod

resources:
  - ../../base

namePrefix: prod-

commonLabels:
  environment: production

# Production-specific image (pinned version)
images:
  - name: inference-service
    newName: ghcr.io/mlifecycle-orchestrator/inference-service
    # Use specific SHA or version tag in production
    newTag: v1.0.0

# Production patches
patches:
  # High availability for production
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: inference-service
      spec:
        replicas: 3
        template:
          spec:
            # Spread across availability zones
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchLabels:
                    app: inference-service
            containers:
              - name: inference
                resources:
                  requests:
                    cpu: "500m"
                    memory: "1Gi"
                  limits:
                    cpu: "2000m"
                    memory: "4Gi"

  # Production HPA
  - patch: |-
      apiVersion: autoscaling/v2
      kind: HorizontalPodAutoscaler
      metadata:
        name: inference-service
      spec:
        minReplicas: 3
        maxReplicas: 50

  # Production PDB
  - patch: |-
      apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: inference-service
      spec:
        minAvailable: 2

configMapGenerator:
  - name: inference-config
    behavior: merge
    literals:
      - LOG_LEVEL=WARNING
      - MODEL_URI=s3://mlifecycle-models-prod/models
