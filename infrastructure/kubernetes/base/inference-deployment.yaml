# =============================================================================
# Inference Service Deployment
# =============================================================================
# Kubernetes deployment for ML inference service.
# Configured for zero-touch deployment with governance compliance.
# =============================================================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-service
  labels:
    app: inference-service
    app.kubernetes.io/component: inference
    app.kubernetes.io/version: "1.0.0"  # Replaced by kustomize overlay
    team: ml-platform
    version: "1.0.0"  # Replaced by kustomize overlay
  annotations:
    description: "Zero-touch deployed ML inference service"
spec:
  replicas: 2
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: inference-service
  template:
    metadata:
      labels:
        app: inference-service
        version: "1.0.0"  # Replaced by kustomize overlay
        team: ml-platform
        app.kubernetes.io/component: inference
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      # Service account with IRSA for S3 access
      serviceAccountName: inference-service

      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault

      # Pod topology spread for high availability
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: inference-service

      # Node affinity for inference workloads
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: workload
                    operator: In
                    values:
                      - inference

      containers:
        - name: inference
          image: ghcr.io/adamatdevops/mlifecycle-orchestrator/inference-service:1.0.0  # Replaced by kustomize overlay
          imagePullPolicy: Always

          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          envFrom:
            - configMapRef:
                name: inference-config

          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace

          # Resource limits
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "2000m"
              memory: "2Gi"
              # nvidia.com/gpu: "1"  # Uncomment for GPU workloads

          # Health checks
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 30  # Allow up to 150s for model loading

          # Security context for container
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL

          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: cache
              mountPath: /.cache

      volumes:
        - name: tmp
          emptyDir: {}
        - name: cache
          emptyDir: {}

      # Graceful shutdown
      terminationGracePeriodSeconds: 30

---
# Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: inference-service
  annotations:
    # IRSA annotation - replaced by Terraform output
    eks.amazonaws.com/role-arn: "arn:aws:iam::ACCOUNT_ID:role/mlifecycle-inference"
